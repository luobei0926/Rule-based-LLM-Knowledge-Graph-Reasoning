{"title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "question": "In paper 'Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology', why does not the approach from English work on other languages?", "answer": "Because, unlike other languages, English does not mark grammatical genders"}
{"title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "question": "How does the method proposed in paper 'Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology' measure grammaticality?", "answer": "by calculating log ratio of grammatical phrase over ungrammatical phrase"}
{"title": "Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language", "question": "How much transcribed data is available for Ainu language?", "answer": "Transcribed data is available for duration of 38h 54m 38s for 8 speakers."}
{"title": "Introducing RONEC -- the Romanian Named Entity Corpus", "question": "What writing styles are present in the corpus proposed in paper 'Introducing RONEC -- the Romanian Named Entity Corpus'?", "answer": "current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials."}
{"title": "Controlling Utterance Length in NMT-based Word Segmentation with Attention", "question": "Does the paper 'Controlling Utterance Length in NMT-based Word Segmentation with Attention' report any alignment-only baseline?", "answer": "Yes"}
{"title": "MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "question": "Does the method proposed in paper 'MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity' beat current state-of-the-art on SICK?", "answer": "No"}
{"title": "MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "question": "How does the method proposed in paper 'MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity' combine MonaLog with BERT?", "answer": "They use Monalog for data-augmentation to fine-tune BERT on this task"}
{"title": "MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "question": "How does the method proposed in paper 'MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity' select monotonicity facts?", "answer": "They derive it from Wordnet"}
{"title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "question": "Can the approach proposed in paper 'Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation' be generalized to other technical domains as well? ", "answer": "There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable."}
{"title": "Pre-Translation for Neural Machine Translation", "question": "Does the method proposed in 'Pre-Translation for Neural Machine Translation' train the NMT model on PBMT outputs?", "answer": "Yes"}
{"title": "SUM-QE: a BERT-based Summary Quality Estimation Model", "question": "What are the model 'SUM-QE' correlation results?", "answer": "High correlation results range from 0.472 to 0.936"}
{"title": "SUM-QE: a BERT-based Summary Quality Estimation Model", "question": "What linguistic quality aspects are addressed in 'SUM-QE: a BERT-based Summary Quality Estimation Model'?", "answer": "Grammaticality, non-redundancy, referential clarity, focus, structure & coherence"}
{"title": "Learning to Describe Phrases with Local and Global Contexts", "question": "Does the method proposed in paper 'Learning to Describe Phrases with Local and Global Contexts' use pretrained word embeddings?", "answer": "Yes"}
{"title": "Simplify the Usage of Lexicon in Chinese NER", "question": "Which are the sequence model architectures the method proposed in paper 'Simplify the Usage of Lexicon in Chinese NER' can be transferred across?", "answer": "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"}
{"title": "Simplify the Usage of Lexicon in Chinese NER", "question": " What percentage of improvement in inference speed is obtained by the method proposed in 'Simplify the Usage of Lexicon in Chinese NER' over the newest state-of-the-art methods?", "answer": "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"}
{"title": "Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis", "question": "Which sentiment analysis tasks are addressed in 'Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis'?", "answer": "12 binary-class classification and multi-class classification of reviews based on rating"}
{"title": "A Stable Variational Autoencoder for Text Modelling", "question": "Does the paper 'A Stable Variational Autoencoder for Text Modelling' compare against state of the art text generation?", "answer": "Yes"}
{"title": "TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus", "question": "How does the semi-automatic construction process in model 'TArC' work?", "answer": "Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus"}
{"title": "TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus", "question": "Does the paper 'TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus' report translation accuracy for an automatic translation model for Tunisian to Arabish words?", "answer": "Yes"}
{"title": "Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches", "question": "What evaluation metrics did the authors use in paper 'Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches'?", "answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."}
{"title": "Select and Attend: Towards Controllable Content Selection in Text Generation", "question": "Does the performance of the model proposed in 'Select and Attend: Towards Controllable Content Selection in Text Generation' necessarily drop when more control is desired?", "answer": "Yes"}
{"title": "Select and Attend: Towards Controllable Content Selection in Text Generation", "question": "How does the model proposed in 'Select and Attend: Towards Controllable Content Selection in Text Generation' perform in comparison to end-to-end headline generation models?", "answer": "Yes"}
{"title": "Modeling Multi-Action Policy for Task-Oriented Dialogues", "question": "What datasets are used in paper 'Modeling Multi-Action Policy for Task-Oriented Dialogues'? ", "answer": "Microsoft Research dataset containing movie, taxi and restaurant domains."}
{"title": "Modeling Multi-Action Policy for Task-Oriented Dialogues", "question": "How better is gCAS approach compared to other approaches?", "answer": "For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52"}
{"title": "Modeling Multi-Action Policy for Task-Oriented Dialogues", "question": "What is specific to gCAS cell?", "answer": "It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner."}
{"title": "Learning with Noisy Labels for Sentence-level Sentiment Classification", "question": "How does the model proposed in 'Learning with Noisy Labels for Sentence-level Sentiment Classification' differ from Generative Adversarial Networks?", "answer": "It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner."}
{"title": "Learning with Noisy Labels for Sentence-level Sentiment Classification", "question": "What is the performance of the model proposed in paper 'Learning with Noisy Labels for Sentence-level Sentiment Classification'?", "answer": "Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)"}
{"title": "Learning with Noisy Labels for Sentence-level Sentiment Classification", "question": "Is the model proposed in paper 'Learning with Noisy Labels for Sentence-level Sentiment Classification' evaluated against a CNN baseline?", "answer": "Yes"}
{"title": "Flexibly-Structured Model for Task-Oriented Dialogues", "question": "How do slot binary classifiers improve performance of flexibly-Structured model for task-oriented dialogues?", "answer": "by adding extra supervision to generate the slots that will be present in the response"}
{"title": "Flexibly-Structured Model for Task-Oriented Dialogues", "question": "What baselines have been used in paper 'Flexibly-Structured Model for Task-Oriented Dialogues'?", "answer": "NDM, LIDM, KVRN, and TSCP/RL"}
{"title": "Learning End-to-End Goal-Oriented Dialog", "question": "How large is the Dialog State Tracking Dataset?", "answer": "1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs"}
{"title": "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack", "question": "What evaluation metric is used in 'Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack'?", "answer": "F1 and Weighted-F1"}
{"title": "Neural Word Segmentation with Rich Pretraining", "question": "What external sources are used in 'Neural Word Segmentation with Rich Pretraining'?", "answer": "Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily"}
{"title": "CamemBERT: a Tasty French Language Model", "question": "Was CamemBERT compared against multilingual BERT on these tasks?", "answer": "Yes"}
{"title": "CamemBERT: a Tasty French Language Model", "question": "How long was CamemBERT trained?", "answer": "Yes"}
{"title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "question": "Does the latent dialogue state help the model proposed in 'Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning'?", "answer": "Yes"}
{"title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "question": "What is the reward model for the reinforcement learning approach proposed in 'Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning'?", "answer": "reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail"}
{"title": "Quantifying Similarity between Relations with Fact Distribution", "question": "Which competitive relational classification models do they test in 'Quantifying Similarity between Relations with Fact Distribution'?", "answer": "For relation prediction they test TransE and for relation extraction they test position aware neural sequence model"}
{"title": "Quantifying Similarity between Relations with Fact Distribution", "question": "How do they gather human judgements for similarity between relations in 'Quantifying Similarity between Relations with Fact Distribution'?", "answer": "By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4"}
{"title": "Multi-style Generative Reading Comprehension", "question": "Does the model proposed in 'Multi-style Generative Reading Comprehension' also take the expected answer style as input?", "answer": "Yes"}
{"title": "Multi-style Generative Reading Comprehension", "question": "What are the baselines that Masque is compared against?", "answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"}
{"title": "Multi-style Generative Reading Comprehension", "question": "What is the performance Masque achieved on NarrativeQA?", "answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"}
{"title": "Multi-style Generative Reading Comprehension", "question": "What is an \"answer style\" mentioned in 'Multi-style Generative Reading Comprehension'?", "answer": "well-formed sentences vs concise answers"}
{"title": "Attention Is (not) All You Need for Commonsense Reasoning", "question": "How does the model proposed in 'Attention Is (not) All You Need for Commonsense Reasoning' differ from BERT?", "answer": "Their model does not differ from BERT."}
{"title": "Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation", "question": "Are synonymous relation taken into account in the Japanese-Vietnamese task in 'Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation'?", "answer": "Yes"}
{"title": "Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation", "question": "Is the supervised morphological learner tested on Japanese in 'Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation'?", "answer": "No"}
{"title": "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction", "question": "What kind of events do 'Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction' extract?", "answer": "No"}
{"title": "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction", "question": "Is 'Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction' the first paper to propose a joint model for event and temporal relation extraction?", "answer": "Yes"}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "How better is accuracy of the model proposed in 'Procedural Reasoning Networks for Understanding Multimodal Procedures' compared to previously reported models?", "answer": "Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59"}
{"title": "Fully Automated Fact Checking Using External Sources", "question": "Does paper 'Fully Automated Fact Checking Using External Sources' report results only on English data?", "answer": "Yes"}
{"title": "Fully Automated Fact Checking Using External Sources", "question": "How are the potentially relevant text fragments identified in 'Fully Automated Fact Checking Using External Sources'?", "answer": " Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable."}
{"title": "Seshat: A tool for managing and verifying annotation campaigns of audio data", "question": "Can Seshat be used for any language?", "answer": "Yes"}
{"title": "Seshat: A tool for managing and verifying annotation campaigns of audio data", "question": "Is Seshat available to the public?", "answer": "Yes"}
{"title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations", "question": "Do they specify the model they use for Gunrock?", "answer": "No"}
{"title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations", "question": "Do they gather explicit user satisfaction data on Gunrock?", "answer": "Yes"}
{"title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition", "question": "How large is the corpus proposed in 'Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition'?", "answer": "It contains 106,350 documents"}
{"title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition", "question": "How large is the dataset proposed in 'Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition'?", "answer": "over 104k documents"}
{"title": "Using Whole Document Context in Neural Machine Translation", "question": "Which language-pair had the better performance in 'Using Whole Document Context in Neural Machine Translation'?", "answer": "French-English"}
{"title": "Can neural networks understand monotonicity reasoning?", "question": "Do they release MED in 'Can neural networks understand monotonicity reasoning'?", "answer": "Yes"}
{"title": "Can neural networks understand monotonicity reasoning?", "question": "How do they define upward and downward reasoning in 'Can neural networks understand monotonicity reasoning'?", "answer": "Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific."}
{"title": "Attention Optimization for Abstractive Document Summarization", "question": "How do 'Attention Optimization for Abstractive Document Summarization' define local variance?", "answer": "The reciprocal of the variance of the attention distribution"}
{"title": "On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation", "question": "How does explicit constraint on the KL divergence term that authors propose looks like?", "answer": "Answer with content missing: (Formula 2) Formula 2 is an answer: \n\\big \\langle\\! \\log p_\\theta({x}|{z}) \\big \\rangle_{q_\\phi({z}|{x})}  -  \\beta |D_{KL}\\big(q_\\phi({z}|{x}) || p({z})\\big)-C|"}
{"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "question": "What datasets are used to evaluate the model 'STransE'?", "answer": "WN18, FB15k"}
{"title": "Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs", "question": "What meta-information is being transferred in 'Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs'?", "answer": "high-order representation of a relation, loss gradient of relation meta"}
{"title": "Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs", "question": "What datasets are used to evaluate the approach proposed in 'Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs'?", "answer": "NELL-One, Wiki-One"}
{"title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER", "question": "Do any of the evaluations show that adversarial learning improves performance in at least two different language families?", "answer": "Yes"}
{"title": "Dialectometric analysis of language variation in Twitter", "question": "Do the authors mention any possible confounds in 'Dialectometric analysis of language variation in Twitter'?", "answer": "Yes"}
{"title": "Dialectometric analysis of language variation in Twitter", "question": "What are the characteristics of the city dialect mentioned in 'Dialectometric analysis of language variation in Twitter'?", "answer": "Lexicon of the cities tend to use most forms of a particular concept"}
{"title": "Dialectometric analysis of language variation in Twitter", "question": "What are the characteristics of the rural dialect mentioned in 'Dialectometric analysis of language variation in Twitter'?", "answer": "It uses particular forms of a concept rather than all of them uniformly"}
{"title": "Learning to Compose Neural Networks for Question Answering", "question": "What benchmark datasets do 'Learning to Compose Neural Networks for Question Answering' use?", "answer": "VQA and GeoQA"}
{"title": "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "question": "Did paper 'A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network' try stacking multiple convolutional layers?", "answer": "No"}
{"title": "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "question": "How many feature maps are generated for a given triple in paper 'A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network'?", "answer": "3 feature maps for a given tuple"}
{"title": "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "question": "How does the number of parameters of the model proposed by 'A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network' compare to other knowledge base completion models?", "answer": "3 feature maps for a given tuple"}
{"title": "Categorization of Semantic Roles for Dictionary Definitions", "question": "How many roles are proposed in 'Categorization of Semantic Roles for Dictionary Definitions'?", "answer": "12"}
{"title": "NIHRIO at SemEval-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in Twitter", "question": "What baseline system is used in 'NIHRIO at SemEval-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in Twitter'?", "answer": "the English version is evaluated. The German version evaluation is in progress "}
{"title": "Fast Domain Adaptation for Neural Machine Translation", "question": "How many examples do 'Fast Domain Adaptation for Neural Machine Translation' have in the target domain?", "answer": "Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)"}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "Does Controlling the Output Length of Neural Machine Translation'' conduct any human evaluation?", "answer": "Yes"}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "How do 'Controlling the Output Length of Neural Machine Translation' enrich the positional embedding with length information", "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "How do 'Controlling the Output Length of Neural Machine Translation' condition the output to a given target-source class?", "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "Does 'Controlling the Output Length of Neural Machine Translation' experiment with combining both methods?", "answer": "Yes"}
{"title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation", "question": "Does they use a neural model for their task in 'Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation'?", "answer": "No"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "How many sentence transformations on average are available per unique sentence in dataset COSTRA 1.0?", "answer": "27.41 transformation on average of single seed sentence is available in dataset."}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "What annotations are available in the dataset COSTRA 1.0?", "answer": "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "How are possible sentence transformations represented in dataset COSTRA 1.0, as new sentences?", "answer": "Yes, as new sentences."}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "What are all 15 types of modifications ilustrated in the dataset COSTRA 1.0?", "answer": "- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "Is this dataset COSTRA 1.0 publicly available?", "answer": "Yes"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "Are some baseline models trained on dataset COSTRA 1.0?", "answer": "Yes"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "question": "Does 'COSTRA 1.0: A Dataset of Complex Sentence Transformations' use external resources to make modifications to sentences?", "answer": "No"}
{"title": "The Transference Architecture for Automatic Post-Editing", "question": "How much is performance hurt when using too small amount of layers in encoder in paper 'The Transference Architecture for Automatic Post-Editing'?", "answer": "comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. "}
{"title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "question": "What are the baseline models experimented with MGNC-CNN?", "answer": "MC-CNN\nMVCNN\nCNN"}
{"title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "question": "By how much of MGNC-CNN out perform the baselines?", "answer": "In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. \nIn case of Irony the difference is about 2.0. \n"}
{"title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "question": "how is quality measured in 'UniSent'?", "answer": "Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality."}
{"title": "Humor Detection: A Transformer Gets the Last Laugh", "question": "What is improvement in accuracy for short Jokes in relation other types of jokes in 'Humor Detection: A Transformer Gets the Last Laugh'?", "answer": "It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%"}
{"title": "Information-Theoretic Probing for Linguistic Structure", "question": "Was any variation in results of 'Information-Theoretic Probing for Linguistic Structure' observed based on language typology?", "answer": "It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information."}
{"title": "Information-Theoretic Probing for Linguistic Structure", "question": "Does the work explicitly study the relationship between model complexity and linguistic structure encoding?", "answer": "No"}
{"title": "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "question": "How do 'Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder' measure the diversity of inferences?", "answer": "by number of distinct n-grams"}
{"title": "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "question": "By how much do ''Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder'' improve the accuracy of inferences over state-of-the-art methods?", "answer": "ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively."}
{"title": "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "question": "How does the context-aware variational autoencoder learn event background information?", "answer": " CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target."}
{"title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "question": "Does the model proposed by 'Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange' beat the baseline models for all the values of the masking parameter tested?", "answer": "No"}
{"title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "question": "Has STES been previously used in the literature to evaluate similar tasks?", "answer": "No"}
{"title": "Meteorologists and Students: A resource for language grounding of geographical descriptors", "question": "Which two datasets does the resource come from in 'Meteorologists and Students: A resource for language grounding of geographical descriptors'?", "answer": "two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor"}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "Do 'Controlling the Output Length of Neural Machine Translation' conduct any human evaluation?", "answer": "Yes"}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "How do 'Controlling the Output Length of Neural Machine Translation' enrich the positional embedding with length information", "answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."}
{"title": "Controlling the Output Length of Neural Machine Translation", "question": "How do 'Controlling the Output Length of Neural Machine Translation' condition the output to a given target-source class?", "answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."}
{"title": "Deep Neural Machine Translation with Linear Associative Unit", "question": "Do 'Controlling the Output Length of Neural Machine Translation' use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?", "answer": "Yes"}
{"title": "Revisiting Low-Resource Neural Machine Translation: A Case Study", "question": "what amounts of size were used on german-english in 'Revisiting Low-Resource Neural Machine Translation: A Case Study'?", "answer": "Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development"}
{"title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention", "question": "Is the layer proposed in 'Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention' smaller in parameters than a Transformer?", "answer": "No"}
{"title": "Transfer Learning Between Related Tasks Using Expected Label Proportions", "question": "How accurate is the aspect based sentiment classifier trained only using the XR loss?", "answer": "BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n"}
{"title": "Universal Dependency Parsing for Hindi-English Code-switching", "question": "How big is the provided treebank in 'Universal Dependency Parsing for Hindi-English Code-switching'?", "answer": "1448 sentences more than the dataset from Bhat et al., 2017"}
{"title": "Finding Dominant User Utterances And System Responses in Conversations", "question": "Does 'Finding Dominant User Utterances And System Responses in Conversations' study frequent user responses to help automate modelling of those?", "answer": "Yes"}
{"title": "Finding Dominant User Utterances And System Responses in Conversations", "question": "How do 'Finding Dominant User Utterances And System Responses in Conversations' generate the synthetic dataset?", "answer": "using generative process"}
{"title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension", "question": "What two types the Chinese reading comprehension dataset consists of in 'Dataset for the First Evaluation on Chinese Machine Reading Comprehension'?", "answer": "cloze-style reading comprehension and user query reading comprehension questions"}
{"title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension", "question": "For which languages most of the existing MRC datasets are created?", "answer": "English"}
{"title": "An Incremental Parser for Abstract Meaning Representation", "question": "Do they use pretrained models as part of their parser in 'Named Entity Recognition with Partially Annotated Training Data'?", "answer": "Yes"}
{"title": "Named Entity Recognition with Partially Annotated Training Data", "question": "What was the F1 score of the model proposed in 'Named Entity Recognition with Partially Annotated Training Data' on the Bengali NER corpus?", "answer": "52.0%"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What models other than standalone BERT is model 'BERTRAM' compared to?", "answer": "Only Bert base and Bert large are compared to proposed approach."}
{"title": "Ask to Learn: A Study on Curiosity-driven Question Generation", "question": "How 'Ask to Learn: A Study on Curiosity-driven Question Generation' evaluate quality of generated output?", "answer": "Through human evaluation where they are asked to evaluate the generated output on a likert scale."}
{"title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "question": "How much improvement does the method proprosed by 'An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation' get over the fine tuning baseline?", "answer": "0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE."}
{"title": "TutorialVQA: Question Answering Dataset for Tutorial Videos", "question": "What evaluation metrics were used in the experiment of 'TutorialVQA'?", "answer": "For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy"}
{"title": "TutorialVQA: Question Answering Dataset for Tutorial Videos", "question": "What kind of instructional videos are in the dataset 'TutorialVQA'?", "answer": "tutorial videos for a photo-editing software"}
{"title": "TutorialVQA: Question Answering Dataset for Tutorial Videos", "question": "What baseline algorithms were presented in 'TutorialVQA: Question Answering Dataset for Tutorial Videos'?", "answer": "a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm"}
{"title": "Open Information Extraction on Scientific Text: An Evaluation", "question": "What is the size of the released dataset in 'Open Information Extraction on Scientific Text: An Evaluation'?", "answer": "440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples."}
{"title": "Open Information Extraction on Scientific Text: An Evaluation", "question": "Which OpenIE systems were used in 'Open Information Extraction on Scientific Text: An Evaluation'?", "answer": "OpenIE4 and MiniIE"}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "What baseline did they compare Entity-GCN to?", "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "How many documents at a time can Entity-GCN handle?", "answer": "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "Did Entity-GCN use a relation extraction method to construct the edges in the graph?", "answer": "No"}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "How did Entity-GCN get relations between mentions?", "answer": "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "How did Entity-GCN detect entity mentions?", "answer": "Exact matches to the entity string and predictions from a coreference resolution system"}
{"title": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "question": "What performance does the Entity-GCN get on WIKIHOP?", "answer": "During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"}
{"title": "Annotating Student Talk in Text-based Classroom Discussions", "question": "how do 'Annotating Student Talk in Text-based Classroom Discussions' measure discussion quality?", "answer": "Measuring three aspects: argumentation, specificity and knowledge domain."}
{"title": "Leveraging Discourse Information Effectively for Authorship Attribution", "question": "How are discourse embeddings analyzed in 'Leveraging Discourse Information Effectively for Authorship Attribution'?", "answer": "They perform t-SNE clustering to analyze discourse embeddings"}
{"title": "Leveraging Discourse Information Effectively for Authorship Attribution", "question": "How are discourse features incorporated into the model proposed by 'Leveraging Discourse Information Effectively for Authorship Attribution'?", "answer": "They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer."}
{"title": "Leveraging Discourse Information Effectively for Authorship Attribution", "question": "What discourse features are used in 'Leveraging Discourse Information Effectively for Authorship Attribution'?", "answer": "Entity grid with grammatical relations and RST discourse relations."}
{"title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "question": "Do they train their own RE model in 'Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping'?", "answer": "Yes"}
{"title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "question": "How big are the datasets of 'Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping'?", "answer": "In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents"}
{"title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "question": "What is new state-of-the-art performance on CoNLL-2009 dataset?", "answer": "In closed setting 84.22 F1 and in open 87.35 F1."}
{"title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "question": "What are two strong baseline methods authors refer to in 'Syntax-Enhanced Self-Attention-Based Semantic Role Labeling'?", "answer": "Marcheggiani and Titov (2017) and Cai et al. (2018)"}
{"title": "Representation Learning for Discovering Phonemic Tone Contours", "question": "How close do clusters match to ground truth tone categories in 'Representation Learning for Discovering Phonemic Tone Contours'?", "answer": "NMI between cluster assignments and ground truth tones for all sylables is:\nMandarin: 0.641\nCantonese: 0.464"}
{"title": "Text-based inference of moral sentiment change", "question": "Does the paper 'Text-based inference of moral sentiment change' discuss previous models which have been applied to the same task?", "answer": "Yes"}
{"title": "Text-based inference of moral sentiment change", "question": "Which datasets are used in 'Text-based inference of moral sentiment change'?", "answer": "Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n"}
{"title": "Text-based inference of moral sentiment change", "question": "How do 'Text-based inference of moral sentiment change' quantify moral relevance?", "answer": "By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence"}
{"title": "Zero-Shot Relation Extraction via Reading Comprehension", "question": "How is the input triple translated to a slot-filling task? in 'Zero-Shot Relation Extraction via Reading Comprehension'?", "answer": "The relation R(x,y) is mapped onto a question q whose answer is y"}
{"title": "Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism", "question": "How do they obtain human judgements in 'Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism'?", "answer": "Using crowdsourcing "}
{"title": "Supervised and Unsupervised Transfer Learning for Question Answering", "question": "How different is the dataset size of source and target in 'Supervised and Unsupervised Transfer Learning for Question Answering'?", "answer": "the training dataset is large while the target dataset is usually much smaller"}
{"title": "Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis", "question": "Which soft-selection approaches are evaluated in 'Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis'?", "answer": "LSTM and BERT "}
{"title": "Cross-Lingual Machine Reading Comprehension", "question": "How big are the datasets used in 'Cross-Lingual Machine Reading Comprehension'?", "answer": "Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified"}
{"title": "Learning to Ask Unanswerable Questions for Machine Reading Comprehension", "question": "Does the approach propoesd by 'Learning to Ask Unanswerable Questions for Machine Reading Comprehension' require a dataset of unanswerable questions mapped to similar answerable questions?", "answer": "Yes"}
{"title": "Pre-Translation for Neural Machine Translation", "question": "Does 'Pre-Translation for Neural Machine Translation' train the NMT model on PBMT outputs?", "answer": "Yes"}
{"title": "Dense Information Flow for Neural Machine Translation", "question": "what datasets were used in 'Dense Information Flow for Neural Machine Translation'?", "answer": "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German"}
{"title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines", "question": "Does the paper 'An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines' mention other works proposing methods to detect anglicisms in Spanish?", "answer": "Yes"}
{"title": "Learning to Automatically Generate Fill-In-The-Blank Quizzes", "question": "What is the size of the dataset of 'Learning to Automatically Generate Fill-In-The-Blank Quizzes'?", "answer": "300,000 sentences with 1.5 million single-quiz questions"}
{"title": "Deep contextualized word representations for detecting sarcasm and irony", "question": "What type of model are the ELMo representations used in?", "answer": "A bi-LSTM with max-pooling on top of it"}
{"title": "Exploring Multilingual Syntactic Sentence Representations", "question": "Do 'Exploring Multilingual Syntactic Sentence Representations' do quantitative quality analysis of learned embeddings?", "answer": "Yes"}
{"title": "SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "question": "How do SIM prevent the model complexity increasing with the increased number of slots?", "answer": "They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN)."}
{"title": "SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "question": "How do SIM measure model size?", "answer": "By the number of parameters."}
{"title": "DENS: A Dataset for Multi-class Emotion Analysis", "question": "How many emotions do they look at in dataset 'DENS'?", "answer": "9"}
{"title": "Named Entity Disambiguation for Noisy Text", "question": "What is the new initialization method proposed in 'Named Entity Disambiguation for Noisy Text'?", "answer": "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data."}
{"title": "Named Entity Disambiguation for Noisy Text", "question": "How was a quality control performed in 'Named Entity Disambiguation for Noisy Text' so that the text is noisy but the annotations are accurate?", "answer": "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."}
{"title": "Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "question": "Which training dataset allowed for the best generalization to benchmark sets in 'Testing the Generalization Power of Neural Network Models Across NLI Benchmarks'?", "answer": "MultiNLI"}
{"title": "Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "question": "Which models were compared in 'Testing the Generalization Power of Neural Network Models Across NLI Benchmarks'?", "answer": "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT"}
{"title": "How do you correct run-on sentences it's not as easy as it seems", "question": "How large is the dataset 'How do you correct run-on sentences it's not as easy as it seems' generate?", "answer": "4.756 million sentences"}
{"title": "Open Information Extraction from Question-Answer Pairs", "question": "Where did 'Open Information Extraction from Question-Answer Pairs' get training data?", "answer": "AmazonQA and ConciergeQA datasets"}
{"title": "Open Information Extraction from Question-Answer Pairs", "question": "What extraction model did 'Open Information Extraction from Question-Answer Pairs' use?", "answer": "Multi-Encoder, Constrained-Decoder model"}
{"title": "Open Information Extraction from Question-Answer Pairs", "question": "Which datasets did 'Open Information Extraction from Question-Answer Pairs' experiment on?", "answer": "ConciergeQA and AmazonQA"}
{"title": "Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation", "question": "what are the three methods presented in 'Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation'?", "answer": "Optimized TF-IDF, iterated TF-IDF, BERT re-ranking."}
{"title": "Learning Multilingual Word Embeddings Using Image-Text Data", "question": "Do the images have multilingual annotations or monolingual ones in 'Learning Multilingual Word Embeddings Using Image-Text Data'?", "answer": "monolingual"}
{"title": "Learning Multilingual Word Embeddings Using Image-Text Data", "question": "How much important is the visual grounding in the learning of the multilingual representations?", "answer": "performance is significantly degraded without pixel data"}
{"title": "Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking", "question": "What is a word confusion network?", "answer": "It is a network used to encode speech lattices to maintain a rich hypothesis space."}
{"title": "The Effect of Context on Metaphor Paraphrase Aptness Judgments", "question": "What document context was added in The Effect of Context on Metaphor Paraphrase Aptness Judgments''?", "answer": "Preceding and following sentence of each metaphor and paraphrase are added as document context"}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "What datasets are used to evaluate the approach proposed by 'Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications'?", "answer": " Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs "}
{"title": "GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception", "question": "How is quality of annotation measured in 'GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception'?", "answer": "Annotators went through various phases to make sure their annotations did not deviate from the mean."}
{"title": "ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation", "question": "Is any data-to-text generation model trained on corpus 'ViGGO', what are the results?", "answer": "Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%."}
{"title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "question": "What are the linguistic differences between each class in 'Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue'?", "answer": "Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes"}
{"title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "question": "Do 'Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion' use any knowledge base to expand abbreviations?", "answer": "Yes"}
{"title": "Transformer-based Cascaded Multimodal Speech Translation", "question": "Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?", "answer": "BLEU scores"}
{"title": "Answering Complex Questions Using Open Information Extraction", "question": "What is the accuracy of the proposed technique in 'Answering Complex Questions Using Open Information Extraction'?", "answer": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge"}
{"title": "Answering Complex Questions Using Open Information Extraction", "question": "Is an entity linking process used in 'Answering Complex Questions Using Open Information Extraction'?", "answer": "No"}
{"title": "Answering Complex Questions Using Open Information Extraction", "question": "Are the OpenIE extractions all triples in 'Answering Complex Questions Using Open Information Extraction'?", "answer": "No"}
{"title": "Answering Complex Questions Using Open Information Extraction", "question": "Can the method proposed by 'Answering Complex Questions Using Open Information Extraction' answer multi-hop questions?", "answer": "Yes"}
{"title": "Facet-Aware Evaluation for Extractive Text Summarization", "question": "What is the problem with existing metrics that 'Facet-Aware Evaluation for Extractive Text Summarization' are trying to address?", "answer": "Answer with content missing: (whole introduction) However, recent\nstudies observe the limits of ROUGE and find in\nsome cases, it fails to reach consensus with human.\njudgment (Paulus et al., 2017; Schluter, 2017)."}
{"title": "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "question": "What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?", "answer": "Humor in headlines (TitleStylist vs Multitask baseline):\nRelevance: +6.53% (5.87 vs 5.51)\nAttraction: +3.72% (8.93 vs 8.61)\nFluency: 1,98% (9.29 vs 9.11)"}
{"title": "Lingke: A Fine-grained Multi-turn Chatbot for Customer Service", "question": "What datasets are used to evaluate the method 'Lingke'?", "answer": "They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,\nincluding chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. "}
{"title": "Lingke: A Fine-grained Multi-turn Chatbot for Customer Service", "question": "What are the results achieved from the method 'Lingke'?", "answer": "Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates."}
{"title": "Subword-augmented Embedding for Cloze Reading Comprehension", "question": "what are the baselines used in 'Subword-augmented Embedding for Cloze Reading Comprehension'?", "answer": "AS Reader, GA Reader, CAS Reader"}
{"title": "Common Voice: A Massively-Multilingual Speech Corpus", "question": "Is audio data per language balanced in dataset 'Common Voice'?", "answer": "No"}
{"title": "Interactive Machine Comprehension with Information Seeking Agents", "question": "What are the models proposed by 'Interactive Machine Comprehension with Information Seeking Agents' evaluated on?", "answer": "They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)"}
{"title": "Analysis of Risk Factor Domains in Psychosis Patient Health Records", "question": "What are their initial results on this task in 'Analysis of Risk Factor Domains in Psychosis Patient Health Records'?", "answer": "Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models."}
{"title": "Weakly-supervised Neural Semantic Parsing with a Generative Ranker", "question": "How does the model proposed by 'Weakly-supervised Neural Semantic Parsing with a Generative Ranker' compute the likelihood of executing to the correction semantic denotation?", "answer": "By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x."}
{"title": "Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "question": "Do they compare to previous models?", "answer": "Yes"}
{"title": "Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "question": "Is the dataset 'Dreaddit' balanced across categories?", "answer": "Yes"}
{"title": "Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "question": "What labels are in the dataset 'Dreaddit'?", "answer": "binary label of stress or not stress"}
{"title": "Cross-lingual Abstract Meaning Representation Parsing", "question": "How is annotation projection done when languages have different word order in 'Cross-lingual Abstract Meaning Representation Parsing'?", "answer": "Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments."}
{"title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization", "question": "Is the baseline a non-heirarchical model like BERT in 'HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization'?", "answer": "There were hierarchical and non-hierarchical baselines; BERT was one of those baselines"}
{"title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "question": "What metric is used to measure performance of 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features'?", "answer": "Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks"}
{"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "question": "does 'Graph Neural Networks with Generated Parameters for Relation Extraction' turn unstructured text inputs to parameters that GNNs can read?", "answer": "Yes"}
{"title": "CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "question": "Is CRWIZ already used for data collection, what are the results?", "answer": "Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant."}
{"title": "Evaluating Rewards for Question Generation Models", "question": "What human evaluation metrics were used in 'Evaluating Rewards for Question Generation Models'?", "answer": "rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context"}
{"title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "question": "did 'BoolQ' use other pretrained language models besides bert?", "answer": "Yes"}
{"title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "question": "how was the dataset 'BoolQ' built?", "answer": "Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes\" or “no\""}
{"title": "Explaining Predictions of Non-Linear Classifiers in NLP", "question": "Do the experiments of 'Explaining Predictions of Non-Linear Classifiers in NLP' explore how various architectures and layers contribute towards certain decisions?", "answer": "No"}